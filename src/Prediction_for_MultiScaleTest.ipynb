{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Mymodel import MyResNet34\n",
    "from Mymodel import MyResNet_CIFAR\n",
    "import tqdm\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms.v2 import (\n",
    "    ToTensor,\n",
    "    RandomHorizontalFlip,\n",
    "    Compose,\n",
    "    RandomCrop,\n",
    "    RandomShortestSize,\n",
    "    AutoAugment,\n",
    "    Normalize,\n",
    "    TenCrop,\n",
    "    CenterCrop,\n",
    "    Pad,\n",
    "    Resize,\n",
    "    Lambda\n",
    ")\n",
    "from torchvision.transforms.autoaugment import AutoAugmentPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset selection\"\"\"\n",
    "# DATASET = \"CIFAR10\"\n",
    "# DATASET = \"CIFAR100\"\n",
    "DATASET = \"ImageNet2012\"\n",
    "\n",
    "\"\"\"Model selection for CIFAR\"\"\"\n",
    "NUM_LAYERS_LEVEL = 5\n",
    "\n",
    "\"\"\"Dataset parameters\"\"\"\n",
    "BATCH = 256\n",
    "SHUFFLE = True\n",
    "NUMOFWORKERS = 8\n",
    "PIN_MEMORY = True\n",
    "SPLIT_RATIO = 0\n",
    "\n",
    "\"\"\"optimizer parameters\"\"\"\n",
    "OPTIMIZER = \"SGD\"\n",
    "# OPTIMIZER = \"Adam\"\n",
    "# OPTIMIZER = \"Adam_decay\"\n",
    "\n",
    "\"\"\"Learning rate scheduler parameters\"\"\"\n",
    "# LOAD_BEFORE_TRAINING = False\n",
    "LOAD_BEFORE_TRAINING = True\n",
    "# NUM_EPOCHS = 1000\n",
    "scheduler_patience_mapping = {\"CIFAR10\": 100, \"CIFAR100\": 100, \"ImageNet2012\": 5}\n",
    "\n",
    "\"\"\"Early stopping parameters\"\"\"\n",
    "# EARLYSTOPPINGPATIENCE = 25\n",
    "file_path = \"\"\n",
    "if DATASET == \"ImageNet2012\":\n",
    "    file_path = f\"{DATASET}/MyResNet34_{BATCH}_{OPTIMIZER}\"\n",
    "else:\n",
    "    file_path = f\"{DATASET}/MyResNet{NUM_LAYERS_LEVEL*6+2}_{BATCH}_{OPTIMIZER}\"\n",
    "\n",
    "if SPLIT_RATIO != 0:\n",
    "    file_path += f\"_{int(SPLIT_RATIO*100)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset:\n",
    "    def __init__(self, root, seceted_dataset, split_ratio=0):\n",
    "        self.Randp = 0.5\n",
    "        self.dataset_name = seceted_dataset\n",
    "        self.split_ratio = split_ratio\n",
    "\n",
    "        if self.dataset_name[:5] == \"CIFAR\":\n",
    "            pass\n",
    "\n",
    "        elif self.dataset_name == \"ImageNet2012\":\n",
    "            self.ImageNetRoot = root + \"/\" + self.dataset_name + \"/\"\n",
    "\n",
    "            \"\"\"\n",
    "            각 지정된 스케일에 따라 10 crop해야하는데, 5개 scale들의 평균을 내야하니까 좀 번거로움.\n",
    "            그치만, 학습 중엔 center crop으로 eval하니, 지금 당장 필요하지는 않음.\n",
    "            \"\"\"\n",
    "\n",
    "            test_data_list = list()\n",
    "            scales = [224, 256, 384, 480, 640]\n",
    "            for scale in scales:\n",
    "                test_data_list.append(\n",
    "                    datasets.ImageFolder(\n",
    "                        root=self.ImageNetRoot + \"val\",\n",
    "                        transform=Compose(\n",
    "                            [\n",
    "                                RandomShortestSize(min_size=scale + 1, antialias=True),\n",
    "                                TenCrop(size=scale),\n",
    "                                ToTensor(),\n",
    "                                Normalize(\n",
    "                                    mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[1, 1, 1],\n",
    "                                    inplace=True,\n",
    "                                ),\n",
    "                            ]\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "                # test_data_list.append(\n",
    "                #     datasets.CIFAR100(\n",
    "                #         root=root,\n",
    "                #         train=True,\n",
    "                #         download=False,\n",
    "                # transform=Compose(\n",
    "                #     [\n",
    "                #         # RandomShortestSize(min_size=scale+1, antialias=True),\n",
    "                #         ToTensor(),\n",
    "                #         TenCrop(size=20),\n",
    "                #     ]\n",
    "                # ),\n",
    "                #     )\n",
    "                # )\n",
    "            self.test_data_list = test_data_list\n",
    "            self.classes = 1000\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dataset: {self.dataset_name}\")\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tmp = LoadDataset(root=\"../data\", seceted_dataset=DATASET)\n",
    "COUNT_OF_CLASSES = tmp.classes\n",
    "test_data = tmp.test_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_list = list()\n",
    "\n",
    "for i in range(5):\n",
    "    test_dataloader_list.append(\n",
    "        DataLoader(\n",
    "            test_data[i],\n",
    "            batch_size=BATCH,\n",
    "            shuffle=SHUFFLE,\n",
    "            # num_workers=NUMOFWORKERS,\n",
    "            # pin_memory=PIN_MEMORY,\n",
    "            # pin_memory_device=\"cuda\",\n",
    "            # persistent_workers=True,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data/ImageNet2012/val\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 RandomShortestSize(min_size=[225], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "                 Normalize(mean=[0.485, 0.456, 0.406], std=[1, 1, 1], inplace=True)\n",
      "           )\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data/ImageNet2012/val\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 RandomShortestSize(min_size=[257], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "                 Normalize(mean=[0.485, 0.456, 0.406], std=[1, 1, 1], inplace=True)\n",
      "           )\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data/ImageNet2012/val\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 RandomShortestSize(min_size=[385], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "                 Normalize(mean=[0.485, 0.456, 0.406], std=[1, 1, 1], inplace=True)\n",
      "           )\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data/ImageNet2012/val\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 RandomShortestSize(min_size=[481], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "                 Normalize(mean=[0.485, 0.456, 0.406], std=[1, 1, 1], inplace=True)\n",
      "           )\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data/ImageNet2012/val\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 RandomShortestSize(min_size=[641], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "                 Normalize(mean=[0.485, 0.456, 0.406], std=[1, 1, 1], inplace=True)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(test_dataloader_list[i].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-34 for ImageNet2012 is loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if DATASET == \"ImageNet2012\":\n",
    "    model = MyResNet34(num_classes=COUNT_OF_CLASSES, Downsample_option=\"B\").to(device)\n",
    "    # model = models.resnet34(pretrained=True).to(device)\n",
    "    # model = models.resnet34(pretrained=False).to(device)\n",
    "    print(f\"ResNet-34 for {DATASET} is loaded.\")\n",
    "else:\n",
    "    model = MyResNet_CIFAR(\n",
    "        num_classes=COUNT_OF_CLASSES, num_layer_factor=NUM_LAYERS_LEVEL\n",
    "    ).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"../models/{file_path}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test0:   8%|▊         | 15/196 [07:08<1:26:12, 28.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(test_dataloader_list[i], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m     12\u001b[0m                 img, labels \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 247\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   3225\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3227\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m   3229\u001b[0m preinit()\n\u001b[1;32m   3231\u001b[0m accept_warnings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "avg_loss = 0\n",
    "avg_acc = 0\n",
    "for i in range(5):\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm.tqdm(test_dataloader_list[i], desc=f\"test{i}\"):\n",
    "            for img in images:\n",
    "                img, labels = img.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(img)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        test_loss /= len(test_dataloader_list[i])\n",
    "        test_acc = correct / total\n",
    "        print(test_loss, test_acc)\n",
    "        avg_loss += test_loss\n",
    "        avg_acc += test_acc\n",
    "\n",
    "print(avg_loss / 5, avg_acc / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test_acc: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test_error: {100 - test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
